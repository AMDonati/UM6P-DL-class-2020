{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "2020-DL-class-01E-nn_tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "OXAq9xIa0Z3A"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZsa3Cb97bxJ"
      },
      "source": [
        "#!pip install torch torchvision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8G3Cg1l0Zzl"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ks5iZCpNAG0t"
      },
      "source": [
        "import torch\n",
        "import math\n",
        "from matplotlib import pyplot\n",
        "import numpy as np\n",
        "from IPython.core.debugger import set_trace"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sujChLjU0Zzu"
      },
      "source": [
        "\n",
        "Introduction to `torch.nn`\n",
        "============================"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_-BwUsN0Zzv"
      },
      "source": [
        "\n",
        "PyTorch provides the modules and classes:\n",
        "* `torch.nn <https://pytorch.org/docs/stable/nn.html>`_ ,: Contains the different layers to be used to create a neural network.\n",
        "* `torch.optim <https://pytorch.org/docs/stable/optim.html>`_ ,: contains the optimisation algorithms to train a neural network.\n",
        "* `Dataset <https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.Dataset>`_ : Allows to create datasets (training, validation and test datasets) with input and target data being **Tensor** objects that can be processed by a Neural Network.  \n",
        "* `DataLoader <https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader>`_: Allows to create **batches** of a dataset to perform **mini-batch gradient descent algorithms. **\n",
        "\n",
        "to help you create and train neural networks.\n",
        "\n",
        "We will first train basic neural net\n",
        "on **the MNIST data** set without using any features from these models; we will\n",
        "initially only use the most basic PyTorch tensor functionality. Then, we will\n",
        "incrementally add one feature from ``torch.nn``, ``torch.optim``, ``Dataset``, or\n",
        "``DataLoader`` at a time, showing exactly what each piece does, and how it\n",
        "works to make the code either more concise, or more flexible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "im9bt4f-TBAB"
      },
      "source": [
        "# Pytorch `Tensor` objects"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JtYawO5TIAK",
        "outputId": "1f282ec4-924f-400b-a170-aecec560c19f"
      },
      "source": [
        "import numpy as np\n",
        "array = np.random.randn(64,1000)\n",
        "print(array.shape)\n",
        "tensor = torch.tensor(array, dtype=torch.float32)\n",
        "print(tensor.shape)\n",
        "print(tensor.dtype)\n",
        "tensor_2 = torch.randn(64, 1000)\n",
        "print(tensor_2.shape), print(tensor_2.size())\n",
        "print(tensor_2.dtype)\n",
        "zero_tensor = torch.zeros(64,10)\n",
        "print(zero_tensor[0])\n",
        "one_tensor = torch.ones(64,10)\n",
        "print(one_tensor[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 1000)\n",
            "torch.Size([64, 1000])\n",
            "torch.float32\n",
            "torch.Size([64, 1000])\n",
            "torch.Size([64, 1000])\n",
            "torch.float32\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e57ScHoHVjTe",
        "outputId": "6a09975c-151e-4390-b589-dead937028d7"
      },
      "source": [
        "# RESIZING\n",
        "x = torch.randn(4, 4)\n",
        "y = x.view(16)\n",
        "z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
        "print(x.size(), y.size(), z.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmPGOzWFXB2N",
        "outputId": "af5c442f-aa85-4ba0-965a-592e6e60294b"
      },
      "source": [
        "# OPERATIONS\n",
        "x = torch.randn(2, 2)\n",
        "print(x)\n",
        "y = torch.randn(2, 2)\n",
        "print(y)\n",
        "print(x + y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.4543,  0.5861],\n",
            "        [-0.9657,  1.1146]])\n",
            "tensor([[-0.8701,  2.1171],\n",
            "        [-0.4888,  0.1595]])\n",
            "tensor([[-1.3243,  2.7032],\n",
            "        [-1.4544,  1.2741]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_sStdzfXW2N",
        "outputId": "150b1d1f-78ec-4e8b-9b89-f8ae691e0b3b"
      },
      "source": [
        "# OPERATIONS - BROADCASTING\n",
        "x = torch.randn(2, 2)\n",
        "print(x)\n",
        "y = torch.ones(2)\n",
        "print(x+y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1.5766, 0.9637],\n",
            "        [0.3108, 0.2199]])\n",
            "tensor([[2.5766, 1.9637],\n",
            "        [1.3108, 1.2199]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TGPP5PMXw4S",
        "outputId": "5999783c-334c-4999-ef99-fe403093cba7"
      },
      "source": [
        "# OPERATIONS - mean, sum\n",
        "x = torch.randn(2, 2)\n",
        "print(x)\n",
        "print(x.mean())\n",
        "print(x.mean(dim=1))\n",
        "print(x.sum())\n",
        "print(x.sum(dim=0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.6575,  0.7742],\n",
            "        [-1.9057,  0.1253]])\n",
            "tensor(-0.0872)\n",
            "tensor([ 0.7159, -0.8902])\n",
            "tensor(-0.3486)\n",
            "tensor([-1.2482,  0.8996])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNFWxw4OYqeI",
        "outputId": "7acdaab5-b178-4c67-b852-33ec283b4f2e"
      },
      "source": [
        "# OPERATIONS - log, exp\n",
        "x = torch.randn(3, 2)\n",
        "print(x)\n",
        "print(x.exp())\n",
        "y = torch.zeros(3,2)\n",
        "print(y.exp())\n",
        "z = torch.ones(2,2)\n",
        "print(z.log())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 3.4353,  0.4304],\n",
            "        [ 0.7875, -0.2651],\n",
            "        [ 1.0675,  0.8801]])\n",
            "tensor([[31.0399,  1.5379],\n",
            "        [ 2.1979,  0.7671],\n",
            "        [ 2.9080,  2.4113]])\n",
            "tensor([[1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]])\n",
            "tensor([[0., 0.],\n",
            "        [0., 0.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbVDKe8kS3el"
      },
      "source": [
        "# MNIST data setup\n",
        "We will use the classic MNIST <https://pytorch.org/docs/stable/torchvision/datasets.html#mnist>_ dataset, which consists of black-and-white images of hand-drawn digits (between 0 and 9).\n",
        "\n",
        "From this dataset, to goal will be to build a classifier able to classify correctly the hand-written digits.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgLjkNVB7kAn"
      },
      "source": [
        "from torchvision.datasets import MNIST"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2m54xg78Oyw"
      },
      "source": [
        "data_path = \"/Users/alicemartin/14_Teaching/UM6P/data\" # put your own path here.\n",
        "mnist_dataset = MNIST(root= data_path, download=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_p4ogCra9HxP",
        "outputId": "1ee512c7-fb75-4dc9-df26-743cddfe50a3"
      },
      "source": [
        "x_train = mnist_dataset.train_data\n",
        "y_train = mnist_dataset.train_labels\n",
        "print(\"x_train type\", x_train.dtype)\n",
        "print(\"x_train shape\", x_train.shape) # (60000, 28, 28)\n",
        "print(\"y_train type\", y_train.dtype)\n",
        "print(\"y_train shape\", y_train.shape) # (60000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train type torch.uint8\n",
            "x_train shape torch.Size([60000, 28, 28])\n",
            "y_train type torch.int64\n",
            "y_train shape torch.Size([60000])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:58: UserWarning: train_data has been renamed data\n",
            "  warnings.warn(\"train_data has been renamed data\")\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:48: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Plkp1sT0Zz6"
      },
      "source": [
        "PyTorch uses ``torch.tensor`` objects. y_train and X_train are such objects. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZXUKpFL0Zz3"
      },
      "source": [
        "Each image is **28 x 28** pixels. Let's plot one of them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gePAKHzq0Zz3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "5c4fc41c-4e61-4a1b-f10e-c5b0051cdb89"
      },
      "source": [
        "pyplot.imshow(x_train[0], cmap=\"gray\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f6b034b45c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTHsBB1u_jQz"
      },
      "source": [
        "For the rest of the tutorial, we will reshape our input data `x_train` from a 2D tensor of size (28*28) to a 1D one of size (784). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Om9nql2p_5gN",
        "outputId": "6c01935c-6817-4869-ced7-8d4a03697570"
      },
      "source": [
        "x_train = x_train.view(x_train.shape[0], -1).float() # convert to a float tensor. \n",
        "print(x_train.shape)\n",
        "print(x_train.dtype)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([48000, 784])\n",
            "torch.float32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvJjX6pN0Zz-"
      },
      "source": [
        "Neural net from scratch (no torch.nn)\n",
        "---------------------------------------------\n",
        "\n",
        "Let's first create a model using nothing but **PyTorch tensor operations**. \n",
        "\n",
        "PyTorch provides methods to create random or zero-filled tensors, which we will\n",
        "use to create **our weights and bias** for a simple linear model. These are just regular\n",
        "tensors, with one very special addition: we tell PyTorch that they require a\n",
        "gradient. This causes PyTorch to record all of the operations done on the tensor,\n",
        "so that it can calculate the gradient during back-propagation *automatically*!\n",
        "\n",
        "For the weights, we set ``requires_grad`` *after* the initialization, since we\n",
        "don't want that step included in the gradient. (Note that a trailling ``_`` in\n",
        "PyTorch signifies that the operation is performed in-place.)\n",
        "\n",
        "We are initializing the weights here with\n",
        "   `Xavier initialisation <http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf>`_\n",
        "   (by multiplying with 1/sqrt(n))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAfFoZm50Zz_"
      },
      "source": [
        "weights = torch.randn(784, 10) / math.sqrt(784) # 784 is the dimension of the input vector, 10 is the dimension of the output vector.\n",
        "weights.requires_grad_()\n",
        "bias = torch.zeros(10, requires_grad=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rClle5hT0Z0C"
      },
      "source": [
        "Thanks to PyTorch's ability to calculate gradients automatically, we can\n",
        "use any standard Python function (or callable object) as a model! \n",
        "So\n",
        "let's just write a plain matrix multiplication and broadcasted addition\n",
        "to create a simple linear model. We also need an activation function, so\n",
        "we'll write `log_softmax` and use it. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXxUp68m76bQ"
      },
      "source": [
        "### Exercice 1: \n",
        "1. Implement the equation of the hidden layer $h=Wx+b$ using the weights and bias defined above by completing the `layer` function below. \n",
        "> **Tip**: You can use the 'torch.matmul' function: \n",
        "https://pytorch.org/docs/stable/generated/torch.matmul.html\n",
        "2. Implement the equation of the softmax output layer using the `logsoftmax` function below: $logsoftmax(x_i) = \\log (e^{x_i} / \\sum_j e^{x_j})$\n",
        "> **Tip**: use the `.exp()` and `.log()` function of pytorch.\n",
        "3. Implement the total model using in the `model` function using the 2 previous functions. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDnl2DJq0Z0D"
      },
      "source": [
        "def layer(xb):\n",
        "  # h= \n",
        "  return h \n",
        "\n",
        "def logsoftmax(x): # this one is better: more stable. \n",
        "\n",
        "def model(xb):"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9YPdxPz0Z0K"
      },
      "source": [
        "We will call\n",
        "our function on one batch of data (in this case, 64 images).  This is\n",
        "one *forward pass*.  Note that our predictions won't be any better than\n",
        "random at this stage, since we start with random weights.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kV4Vu3tq0Z0M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85515e4c-b23c-4e87-8c05-1c06e666ad3f"
      },
      "source": [
        "n = x_train.shape[0] # number of training samples\n",
        "bs = 64  # batch size\n",
        "xb = x_train[0:bs]  # a mini-batch from x\n",
        "logpreds = model(xb)  # predictions\n",
        "logpreds[0], logpreds.shape\n",
        "print(logpreds[0], logpreds.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
            "       grad_fn=<SelectBackward>) torch.Size([64, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DlepBKR0Z0P"
      },
      "source": [
        "As you see, the ``logpreds`` tensor contains not only the tensor values, but also a\n",
        "gradient function. We'll use this later to do backprop.\n",
        "\n",
        "Let's implement negative log-likelihood to use as the loss function\n",
        "(again, we can just use standard Python):\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2Ix1_hL9h3T"
      },
      "source": [
        "### Exercice 2: \n",
        " Implement the negative log-likelihood (equivalent to the cross-entropy in the classification setting) using the `nll` function below:  \n",
        "\n",
        " $nll = - 1/N \\sum_i * \\log f(x_i)[y_i]$  \n",
        " With $N$ the number of training samples in the batch, $y_i$ the target value for the i-th sample, and $f(x_i)$ the ouput of the neural network for the i-th training input sample of the batch. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CUFplT30Z0P"
      },
      "source": [
        "def nll(logpred, target):\n",
        "  # nll = \n",
        "  return nll\n",
        "\n",
        "loss_func = nll"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dq6H3ok_0Z0S"
      },
      "source": [
        "Let's check our loss with our random model, so we can see if we improve\n",
        "after a backprop pass later.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaH4jo_00Z0T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e24e1fb3-ad13-4557-bbb5-f95f3b0c486d"
      },
      "source": [
        "yb = y_train[0:bs]\n",
        "print(loss_func(logpreds, yb))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(inf, grad_fn=<NegBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEksa0_l0Z0V"
      },
      "source": [
        "### Exercice 3: Implement the accuracy function.\n",
        "Let's also implement a function to calculate the accuracy of our model.  \n",
        "\n",
        "For each prediction, if the index with the largest value matches the\n",
        "target value, then the prediction was correct.\n",
        "\n",
        "**Tip**: you can use the 'torch.argmax' function: https://pytorch.org/docs/stable/generated/torch.argmax.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHoFNmWd0Z0W"
      },
      "source": [
        "def accuracy(out, yb):\n",
        "    # acc= \n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkOP7PQw0Z0Z"
      },
      "source": [
        "Let's check the accuracy of our random model, so we can see if our\n",
        "accuracy improves as our loss improves.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_S1zbeA0Z0Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14b26fa7-936f-4cde-dd30-9cbb448918b3"
      },
      "source": [
        "print(accuracy(logpreds, yb))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.1250)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3fV8gIk0Z0c"
      },
      "source": [
        "We can now run a training loop.  For each iteration, we will:\n",
        "\n",
        "- select a mini-batch of data (of size ``bs``)\n",
        "- use the model to make predictions\n",
        "- calculate the loss\n",
        "- ``loss.backward()`` updates the gradients of the model, in this case, ``weights``\n",
        "  and ``bias``.\n",
        "\n",
        "We now use these gradients to update the weights and bias.  We do this\n",
        "within the ``torch.no_grad()`` context manager, because we do not want these\n",
        "actions to be recorded for our next calculation of the gradient.  You can read\n",
        "more about how PyTorch's Autograd records operations\n",
        "`here <https://pytorch.org/docs/stable/notes/autograd.html>`_.\n",
        "\n",
        "We then set the\n",
        "gradients to zero, so that we are ready for the next loop.\n",
        "Otherwise, our gradients would record a running tally of all the operations\n",
        "that had happened (i.e. ``loss.backward()`` *adds* the gradients to whatever is\n",
        "already stored, rather than replacing them)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5hqUBpX0Z0e"
      },
      "source": [
        "lr = 0.5  # learning rate\n",
        "epochs = 2  # how many epochs to train for\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for i in range((n - 1) // bs + 1):\n",
        "        start_i = i * bs\n",
        "        end_i = start_i + bs\n",
        "        xb = x_train[start_i:end_i] # mini-batch input data \n",
        "        yb = y_train[start_i:end_i] # mini-batch target data\n",
        "        pred = model(xb)\n",
        "        loss = loss_func(pred, yb)\n",
        "\n",
        "        loss.backward()\n",
        "        with torch.no_grad():\n",
        "          # manual update of the model parameters (weights and bias) using Stochastic Gradient Descent (SGD)\n",
        "            weights -= weights.grad * lr\n",
        "            bias -= bias.grad * lr\n",
        "            weights.grad.zero_() # resetting the gradients to 0.\n",
        "            bias.grad.zero_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5jbJq5h0Z0g"
      },
      "source": [
        "That's it: we've created and trained a minimal neural network (in this case, a\n",
        "logistic regression, since we have no hidden layers) entirely from scratch!\n",
        "\n",
        "Let's check the loss and accuracy and compare those to what we got\n",
        "earlier. We expect that the loss will have decreased and accuracy to\n",
        "have increased, and they have.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTzoI1Q10Z0i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d47e51b3-832c-4c22-fab1-252f035c806e"
      },
      "source": [
        "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(nan, grad_fn=<NegBackward>) tensor(0.0625)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b3De6Hb0Z0m"
      },
      "source": [
        "Using torch.nn.functional\n",
        "------------------------------\n",
        "\n",
        "We will now refactor our code, so that it does the same thing as before, only\n",
        "we'll start taking advantage of PyTorch's ``nn`` classes to make it more concise\n",
        "and flexible.\n",
        "\n",
        "The first and easiest step is to make our code shorter by replacing our\n",
        "hand-written activation and loss functions with those from ``torch.nn.functional``\n",
        "(which is generally imported into the namespace ``F`` by convention). This module\n",
        "contains all the functions in the ``torch.nn`` library (whereas other parts of the\n",
        "library contain classes).  \n",
        "\n",
        "If you're using **negative log likelihood loss** and **log softmax activation**,\n",
        "then Pytorch provides a single function ``F.cross_entropy`` that combines\n",
        "the two. So we can even remove the activation function from our model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bfkp_5lg0Z0n"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "loss_func = F.cross_entropy\n",
        "\n",
        "def model(xb):\n",
        "    return torch.matmul(xb, weights) + bias"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFkrRUxq0Z0q"
      },
      "source": [
        "Note that we no longer call ``log_softmax`` in the ``model`` function. Let's\n",
        "confirm that our loss and accuracy are the same as before:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XK5cgR5I0Z0r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3afc8ef4-c8b0-4455-c693-84d69c2f4daf"
      },
      "source": [
        "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(nan, grad_fn=<NllLossBackward>) tensor(0.0625)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdlc9xAv0Z0u"
      },
      "source": [
        "Refactor using nn.Module\n",
        "-----------------------------\n",
        "Next up, we'll use ``nn.Module`` and ``nn.Parameter``, for a clearer and more\n",
        "concise training loop. We subclass ``nn.Module`` (which itself is a class and\n",
        "able to keep track of state).  In this case, we want to create a class that\n",
        "holds our weights, bias, and method for the forward step.  ``nn.Module`` has a\n",
        "number of attributes and methods (such as ``.parameters()`` and ``.zero_grad()``)\n",
        "which we will be using.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suVBA5ns0Z0v"
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "class Mnist_Logistic(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784))\n",
        "        self.bias = nn.Parameter(torch.zeros(10))\n",
        "\n",
        "    def forward(self, xb):\n",
        "        return torch.matmul(xb, weights) + bias"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwI_I3gM0Z0y"
      },
      "source": [
        "Since we're now using an object instead of just using a function, we\n",
        "first have to instantiate our model:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MytGQVL0Z0y"
      },
      "source": [
        "model = Mnist_Logistic()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iM0dKzj0Z02"
      },
      "source": [
        "Now we can calculate the loss in the same way as before. Note that\n",
        "``nn.Module`` objects are used as if they are functions (i.e they are\n",
        "*callable*), but behind the scenes Pytorch will call our ``forward``\n",
        "method automatically.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TDUtAmn0Z03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98d83818-ce15-475b-8f29-d36236cfb979"
      },
      "source": [
        "print(loss_func(model(xb), yb))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(137.5097, grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jah4FWZw0Z06"
      },
      "source": [
        "Previously for our training loop we had to update the values for each parameter\n",
        "by name, and manually zero out the grads for each parameter separately, like this:\n",
        "::\n",
        "\n",
        "  `with torch.no_grad():`  \n",
        "\n",
        "      weights -= weights.grad * lr\n",
        "      bias -= bias.grad * lr\n",
        "      weights.grad.zero_()\n",
        "      bias.grad.zero_()\n",
        "\n",
        "\n",
        "Now we can take advantage of `model.parameters()` and `model.zero_grad()` (which\n",
        "are both defined by PyTorch for ``nn.Module``) to make those steps more concise\n",
        "and less prone to the error of forgetting some of our parameters, particularly\n",
        "if we had a more complicated model:\n",
        "::\n",
        "  `with torch.no_grad():`\n",
        "\n",
        "      for p in model.parameters(): p -= p.grad * lr\n",
        "      model.zero_grad()\n",
        "\n",
        "\n",
        "We'll wrap our little training loop in a ``fit`` function so we can run it\n",
        "again later.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PG2bjfX70Z07"
      },
      "source": [
        "def fit():\n",
        "    for epoch in range(epochs):\n",
        "        for i in range((n - 1) // bs + 1):\n",
        "            start_i = i * bs\n",
        "            end_i = start_i + bs\n",
        "            xb = x_train[start_i:end_i]\n",
        "            yb = y_train[start_i:end_i]\n",
        "            pred = model(xb)\n",
        "            loss = loss_func(pred, yb)\n",
        "\n",
        "            loss.backward()\n",
        "            with torch.no_grad():\n",
        "              # manual update of all the parameters using SGD\n",
        "                for p in model.parameters():\n",
        "                    p -= p.grad * lr\n",
        "                model.zero_grad()\n",
        "\n",
        "fit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHtENrW30Z0-"
      },
      "source": [
        "Let's double-check that our loss has gone down:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7dBXoHW0Z0_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5af5d144-dfa0-4123-98f2-787f21561bc7"
      },
      "source": [
        "print(loss_func(model(xb), yb))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0., grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owOCbRsQ0Z1D"
      },
      "source": [
        "Refactor using nn.Linear\n",
        "-------------------------\n",
        " Instead of manually defining and\n",
        "initializing ``self.weights`` and ``self.bias``, and calculating ``xb  @\n",
        "self.weights + self.bias``, we will instead use the Pytorch class\n",
        "`nn.Linear <https://pytorch.org/docs/stable/nn.html#linear-layers>`_ for a\n",
        "linear layer, which does all that for us. \n",
        "Pytorch has many types of\n",
        "predefined layers that can greatly simplify our code, and often makes it\n",
        "faster too.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cee0YQwX_9KA"
      },
      "source": [
        "### Exercice 4. \n",
        "Using the Class `Mnist_Logistic` below, implement: \n",
        "1. Inside the `def` function a nn.Linear layer with the same dimensions as above.\n",
        "2. Inside the `forward` function (corresponding to a forward pass), the forward pass of this linear layer on input data $x_b$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4eMqUVd0Z1E"
      },
      "source": [
        "class Mnist_Logistic(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        #self.lin = \n",
        "\n",
        "    def forward(self, xb):"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbcXbuvj0Z1I"
      },
      "source": [
        "We instantiate our model and calculate the loss in the same way as before:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUFOVuHk0Z1K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75c0d34c-5ac6-43f6-9cab-6d16c481cf38"
      },
      "source": [
        "model = Mnist_Logistic()\n",
        "print(loss_func(model(xb), yb))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(78.9874, grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8y-WUNQ0Z1N"
      },
      "source": [
        "We are still able to use our same ``fit`` method as before.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Em8NRSdy0Z1R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84b66fcd-12a2-4f4b-a961-8d233d748066"
      },
      "source": [
        "fit()\n",
        "\n",
        "print(loss_func(model(xb), yb))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0., grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UOv_wfE0Z1T"
      },
      "source": [
        "Refactor using optim\n",
        "------------------------------\n",
        "\n",
        "Pytorch also has a package with various optimization algorithms, ``torch.optim``.  \n",
        "\n",
        "We can use the ``step`` method from our optimizer to take a forward step, instead\n",
        "of manually updating each parameter.\n",
        "\n",
        "This will let us replace our previous manually coded optimization step:\n",
        "::  \n",
        "\n",
        "  `with torch.no_grad():`\n",
        "\n",
        "      for p in model.parameters(): \n",
        "      p -= p.grad * lr \n",
        "      model.zero_grad()\n",
        "\n",
        "and instead use just:\n",
        ":: \n",
        "\n",
        "  `opt.step()`  \n",
        "  `opt.zero_grad()`\n",
        "\n",
        "(``optim.zero_grad()`` resets the gradient to 0 and we need to call it before\n",
        "computing the gradient for the next minibatch.)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRcbCIAr0Z1U"
      },
      "source": [
        "from torch import optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuGUdX5U0Z1W"
      },
      "source": [
        "We'll define a little function to create our model and optimizer so we\n",
        "can reuse it in the future.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUDBXaHu0Z1X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3953331b-b197-4b50-a5f3-68bd00353e33"
      },
      "source": [
        "def get_model():\n",
        "    model = Mnist_Logistic()\n",
        "    return model, optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "model, opt = get_model()\n",
        "print(loss_func(model(xb), yb))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(66.1239, grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tWGWmIGCFe8"
      },
      "source": [
        "### Exercice 5\n",
        "Refactor the function `fit` implemented above using `opt.step()` & `opt.zero_grad()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3xPyYuaBpSl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f48f28da-be23-4324-df9f-9ff3e9808146"
      },
      "source": [
        "def fit():\n",
        "  pass\n",
        "fit()\n",
        "print(loss_func(model(xb), yb))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0., grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-FBZlBH0Z1a"
      },
      "source": [
        "Refactor using Dataset\n",
        "------------------------------\n",
        "\n",
        "PyTorch has an abstract Dataset class.  \n",
        "A Dataset can be anything that has\n",
        "a ``__len__`` function (called by Python's standard ``len`` function) and\n",
        "a ``__getitem__`` function as a way of indexing into it.  \n",
        "\n",
        "`This tutorial <https://pytorch.org/tutorials/beginner/data_loading_tutorial.html>`_\n",
        "walks through a nice example of creating a custom ``FacialLandmarkDataset`` class\n",
        "as a subclass of ``Dataset``.\n",
        "\n",
        "PyTorch's `TensorDataset <https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#TensorDataset>`_\n",
        "is a Dataset wrapping tensors. \n",
        "By defining a length and way of indexing,\n",
        "this also gives us a way to iterate, index, and slice along the first\n",
        "dimension of a tensor. This will make it easier to access both the\n",
        "independent and dependent variables in the same line as we train."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmjTG6oD0Z1b"
      },
      "source": [
        "from torch.utils.data import TensorDataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2mxmuTJ0Z1e"
      },
      "source": [
        "Both ``x_train`` and ``y_train`` can be combined in a single ``TensorDataset``,\n",
        "which will be easier to iterate over and slice.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQjeMxlk0Z1e"
      },
      "source": [
        "train_ds = TensorDataset(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vRi0wrx0Z1i"
      },
      "source": [
        "Previously, we had to iterate through minibatches of x and y values separately:\n",
        "::\n",
        "    xb = x_train[start_i:end_i]\n",
        "    yb = y_train[start_i:end_i]\n",
        "\n",
        "\n",
        "Now, we can do these two steps together:\n",
        "::\n",
        "    xb,yb = train_ds[i*bs : i*bs+bs]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuJOwMZb0Z1i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90f056ae-e6f7-4355-dafa-da3cc927bcbe"
      },
      "source": [
        "model, opt = get_model()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for i in range((n - 1) // bs + 1):\n",
        "        xb, yb = train_ds[i * bs: i * bs + bs]\n",
        "        pred = model(xb)\n",
        "        loss = loss_func(pred, yb)\n",
        "\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "\n",
        "print(loss_func(model(xb), yb))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0., grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2HkxD9D0Z1m"
      },
      "source": [
        "Refactor using DataLoader\n",
        "------------------------------\n",
        "\n",
        "Pytorch's ``DataLoader`` is responsible for managing batches. You can\n",
        "create a ``DataLoader`` from any ``Dataset``. \n",
        "``DataLoader`` makes it easier\n",
        "to iterate over batches. Rather than having to use ``train_ds[i*bs : i*bs+bs]``,\n",
        "the DataLoader gives us each minibatch automatically."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6W3BpTW0Z1n"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_ds = TensorDataset(x_train, y_train)\n",
        "train_dl = DataLoader(train_ds, batch_size=bs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVx1c1d90Z1s"
      },
      "source": [
        "Previously, our loop iterated over batches (xb, yb) like this:\n",
        "::\n",
        "      for i in range((n-1)//bs + 1):\n",
        "          xb,yb = train_ds[i*bs : i*bs+bs]\n",
        "          pred = model(xb)\n",
        "\n",
        "Now, our loop is much cleaner, as (xb, yb) are loaded automatically from the data loader:\n",
        "::\n",
        "      for xb,yb in train_dl:\n",
        "          pred = model(xb)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Bbgjc110Z1s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "359f5fd0-baf5-4a05-80db-aa0e6df2f0b9"
      },
      "source": [
        "model, opt = get_model()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for xb, yb in train_dl:\n",
        "        pred = model(xb)\n",
        "        loss = loss_func(pred, yb)\n",
        "\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "\n",
        "print(loss_func(model(xb), yb))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0., grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I10OMHNO0Z1v"
      },
      "source": [
        "Thanks to Pytorch's ``nn.Module``, ``nn.Parameter``, ``Dataset``, and ``DataLoader``,\n",
        "our training loop is now dramatically smaller and easier to understand. Let's\n",
        "now try to add the basic features necessary to create effecive models in practice.\n",
        "\n",
        "Add validation\n",
        "-----------------------\n",
        "\n",
        "In section 1, we were just trying to get a reasonable training loop set up for\n",
        "use on our training data.  In reality, you **always** should also have\n",
        "a `validation set <https://www.fast.ai/2017/11/13/validation-sets/>`_, in order\n",
        "to identify if you are overfitting.\n",
        "\n",
        "Shuffling the training data is\n",
        "`important <https://www.quora.com/Does-the-order-of-training-data-matter-when-training-neural-networks>`_\n",
        "to prevent correlation between batches and overfitting. On the other hand, the\n",
        "validation loss will be identical whether we shuffle the validation set or not.\n",
        "Since shuffling takes extra time, it makes no sense to shuffle the validation data.\n",
        "\n",
        "We'll use a batch size for the validation set that is twice as large as\n",
        "that for the training set. This is because the validation set does not\n",
        "need backpropagation and thus takes less memory (it doesn't need to\n",
        "store the gradients). We take advantage of this to use a larger batch\n",
        "size and compute the loss more quickly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8g8WfCGXQ1WK"
      },
      "source": [
        "### Exercice 5: \n",
        "1. Split the training dataset between a train dataset and a validation dataset.   \n",
        "You can use for example the `train_test_split` function of sci-kit learn. \n",
        "\n",
        "2. Create a train TensorDataset, a validation TensorDataset, a train DataLoader, and a validation DataLoader."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7wnro_IDBKy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34f13432-8096-4200-cd0e-c566b3e1fc14"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([48000, 784])\n",
            "torch.Size([12000, 784])\n",
            "torch.Size([48000])\n",
            "torch.Size([12000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTTIxkRv0Z1v"
      },
      "source": [
        "#train_ds = \n",
        "#train_dl = \n",
        "\n",
        "#valid_ds =\n",
        "#valid_dl ="
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzgidzOT0Z1y"
      },
      "source": [
        "We will calculate and print the validation loss at the end of each epoch.\n",
        "\n",
        "(Note that we always call ``model.train()`` before training, and ``model.eval()``\n",
        "before inference, because these are used by layers such as ``nn.BatchNorm2d``\n",
        "and ``nn.Dropout`` to ensure appropriate behaviour for these different phases.)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zK7g8gVB0Z1z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48061c2e-0751-4da5-e7b0-b17fcf19fb59"
      },
      "source": [
        "model, opt = get_model()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for xb, yb in train_dl:\n",
        "        pred = model(xb)\n",
        "        loss = loss_func(pred, yb)\n",
        "\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        valid_loss = sum(loss_func(model(xb), yb) for xb, yb in valid_dl)\n",
        "\n",
        "    print(epoch, valid_loss / len(valid_dl))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 tensor(8295.5615)\n",
            "1 tensor(6910.6675)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mx1ZuHK0Z11"
      },
      "source": [
        "Create fit() and get_data()\n",
        "----------------------------------\n",
        "\n",
        "We'll now do a little refactoring of our own. Since we go through a similar\n",
        "process twice of calculating the loss for both the training set and the\n",
        "validation set, let's make that into its own function, ``loss_batch``, which\n",
        "computes the loss for one batch.\n",
        "\n",
        "We pass an optimizer in for the training set, and use it to perform\n",
        "backprop.  For the validation set, we don't pass an optimizer, so the\n",
        "method doesn't perform backprop.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9yQqyRk0Z11"
      },
      "source": [
        "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
        "    loss = loss_func(model(xb), yb)\n",
        "\n",
        "    if opt is not None:\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "\n",
        "    return loss.item(), len(xb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAMEoS0W0Z14"
      },
      "source": [
        "``fit`` runs the necessary operations to train our model and compute the\n",
        "training and validation losses for each epoch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HoqqoDT0Z14"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for xb, yb in train_dl:\n",
        "            loss_batch(model, loss_func, xb, yb, opt)\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            losses, nums = zip(\n",
        "                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n",
        "            )\n",
        "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
        "\n",
        "        print(epoch, val_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsROasjH0Z2A"
      },
      "source": [
        "``get_data`` returns dataloaders for the training and validation sets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EC-pm9x10Z2B"
      },
      "source": [
        "def get_data(train_ds, valid_ds, bs):\n",
        "    return (\n",
        "        DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
        "        DataLoader(valid_ds, batch_size=bs * 2),\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sW0YZ49M0Z2H"
      },
      "source": [
        "Now, our whole process of obtaining the data loaders and fitting the\n",
        "model can be run in 3 lines of code:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHKGhUzA0Z2I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "020bae15-167d-477e-a46b-c667e65680e9"
      },
      "source": [
        "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
        "model, opt = get_model()\n",
        "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 8078.89233984375\n",
            "1 7892.56967578125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXAq9xIa0Z3A"
      },
      "source": [
        "Closing thoughts\n",
        "-----------------\n",
        "\n",
        "We now have a general data pipeline and training loop which you can use for\n",
        "training many types of models using Pytorch. To see how simple training a model\n",
        "can now be, take a look at the `mnist_sample` sample notebook.\n",
        "\n",
        "Of course, there are many things you'll want to add, such as data augmentation,\n",
        "hyperparameter tuning, monitoring training, transfer learning, and so forth.\n",
        "These features are available in the fastai library, which has been developed\n",
        "using the same design approach shown in this tutorial, providing a natural\n",
        "next step for practitioners looking to take their models further.\n",
        "\n",
        "We promised at the start of this tutorial we'd explain through example each of\n",
        "``torch.nn``, ``torch.optim``, ``Dataset``, and ``DataLoader``. So let's summarize\n",
        "what we've seen:\n",
        "\n",
        " - **torch.nn**\n",
        "\n",
        "   + ``Module``: creates a callable which behaves like a function, but can also\n",
        "     contain state(such as neural net layer weights). It knows what ``Parameter`` (s) it\n",
        "     contains and can zero all their gradients, loop through them for weight updates, etc.\n",
        "   + ``Parameter``: a wrapper for a tensor that tells a ``Module`` that it has weights\n",
        "     that need updating during backprop. Only tensors with the `requires_grad` attribute set are updated\n",
        "   + ``functional``: a module(usually imported into the ``F`` namespace by convention)\n",
        "     which contains activation functions, loss functions, etc, as well as non-stateful\n",
        "     versions of layers such as convolutional and linear layers.\n",
        " - ``torch.optim``: Contains optimizers such as ``SGD``, which update the weights\n",
        "   of ``Parameter`` during the backward step\n",
        " - ``Dataset``: An abstract interface of objects with a ``__len__`` and a ``__getitem__``,\n",
        "   including classes provided with Pytorch such as ``TensorDataset``\n",
        " - ``DataLoader``: Takes any ``Dataset`` and creates an iterator which returns batches of data.\n",
        "\n"
      ]
    }
  ]
}